---
title: "patent"
output: html_document
date: "2023-03-09"
---
### Required Library 
##Install library
```{r}
#install.packages("patentsview")
#install.packages("tidyr")
#install.packages("visNetwork")
#install.packages("magrittr")
#install.packages("stringr")
#install.packages("knitr")
#install.packages("tm")
#install.packages("wordcloud")
#install.packages("topicmodels")
#install.packages("tidytext")
#install.packages("reshape2")
#install.packages("ggplot2")
#install.packages("dplyr")
#install.packages("stringi")
#install.packages("LDAvis")
#install.packages("servr")

```

## Import library
```{r}
library(patentsview)
library(tidyr)
library(dplyr)
library(visNetwork)
library(magrittr)
library(stringr)
library(knitr)
library(tm)
library(wordcloud)
library(topicmodels)
library(tidytext)
library(reshape2)
library(ggplot2)
library(dplyr)
library(stringi)
library(LDAvis)
library(servr)
```


### Data collecting
## Query
```{r}
query_hev <-
  with_qfuns(
    and(
      gte(patent_date = "1978-01-01"),
      or(
        text_all(patent_abstract="vehicle"),
        text_all(patent_abstract="car"),
        text_all(patent_abstract="automobile"),
        text_all(patent_abstract="bus"),
        text_all(patent_abstract="lorry"),
        text_all(patent_abstract="truck")
      ),
      or(
        text_all(patent_abstract="hybrid vehicle"),
        text_all(patent_abstract="hybrid electric vehicle"),
        text_all(patent_abstract="hybrid propulsion")
      ),
      or(
        begins(cpc_subgroup_id = "B60K6"),
        begins(cpc_subgroup_id = "B60L7"),
        begins(cpc_subgroup_id = "B60W20"),
        begins(cpc_subgroup_id = "B60L7/20")
      )
    )
  )
fields<-c("patent_number" , "assignee_organization", "patent_title","patent_num_combined_citations","patent_abstract","cited_patent_number", "citedby_patent_number","patent_date")


query_bev <-
  with_qfuns(
    and(
      gte(patent_date = "1978-01-01"),
      or(
        text_all(patent_abstract="vehicle"),
        text_all(patent_abstract="car"),
        text_all(patent_abstract="automobile"),
        text_all(patent_abstract="bus"),
        text_all(patent_abstract="lorry"),
        text_all(patent_abstract="truck")
      ),
      or(
        text_all(patent_abstract="electric vehicle"),
        text_all(patent_abstract="electric car"),
        text_all(patent_abstract="electric automobile"),
        text_all(patent_abstract="plug in electric")
      ),
      or(
        begins(cpc_subgroup_id = "B60L3"),
        begins(cpc_subgroup_id = "B60L15"),
        begins(cpc_subgroup_id = "B60K1"),
        begins(cpc_subgroup_id = "B60W10/08"),
        begins(cpc_subgroup_id = "B60W10/24"),
        begins(cpc_subgroup_id = "B60W10/26")
      )
    )
  )
fields<-c("patent_number" , "assignee_organization", "patent_title","patent_num_combined_citations","patent_abstract","cited_patent_number", "citedby_patent_number","patent_date")

query_fcev <-
  with_qfuns(
    and(
      gte(patent_date = "1978-01-01"),
      or(
        text_all(patent_abstract="vehicle"),
        text_all(patent_abstract="car"),
        text_all(patent_abstract="automobile"),
        text_all(patent_abstract="bus"),
        text_all(patent_abstract="lorry"),
        text_all(patent_abstract="truck")
      ),
      or(
        text_all(patent_abstract="fuel cell")
      ),
      or(
        begins(cpc_subgroup_id = "B60W10/28"),
        begins(cpc_subgroup_id = "H01M8")
      )
    )
  )
fields<-c("patent_number" , "assignee_organization", "patent_title","patent_num_combined_citations","patent_abstract","cited_patent_number", "citedby_patent_number","patent_date")


query_icev <-
  with_qfuns(
    and(
      gte(patent_date = "1978-01-01"),
      or(
        text_all(patent_abstract="vehicle"),
        text_all(patent_abstract="car"),
        text_all(patent_abstract="automobile"),
        text_all(patent_abstract="bus"),
        text_all(patent_abstract="lorry"),
        text_all(patent_abstract="truck")
      ),
      or(
        text_all(patent_abstract="internal combustion"),
        text_all(patent_abstract="ic engine"),
        text_all(patent_abstract="diesel engine")
      ),
      or(
        begins(cpc_subgroup_id = "F02B"),
        begins(cpc_subgroup_id = "F02D"),
        begins(cpc_subgroup_id = "F02F"),
        begins(cpc_subgroup_id = "F02M"),
        begins(cpc_subgroup_id = "F02N"),
        begins(cpc_subgroup_id = "F02P"),
        begins(cpc_subgroup_id = "H01M8")
      )
    )
  )
fields<-c("patent_number" , "assignee_organization", "patent_title","patent_num_combined_citations","patent_abstract","cited_patent_number", "citedby_patent_number","patent_date")
```

## Request patentsview API
```{r}
pv_out_hev <- search_pv(query=query_hev , fields = fields , all_pages=TRUE)
pv_out_bev <- search_pv(query=query_bev , fields = fields , all_pages=TRUE)
pv_out_fcev <- search_pv(query=query_fcev , fields = fields , all_pages=TRUE)
pv_out_icev <- search_pv(query=query_icev , fields = fields , all_pages=TRUE)
```

## Cast data
```{r}
fcev_data <-pv_out_fcev$data$patents %>%
  unnest(patent_number) 

bev_data <- pv_out_bev$data$patents %>%
  unnest(patent_number) 

hev_data <- pv_out_hev$data$patents %>%
  unnest(patent_number) 

icev_data <- pv_out_icev$data$patents %>%
  unnest(patent_number) 

rm(pv_out_bev)
rm(pv_out_fcev)
rm(pv_out_hev)
rm(pv_out_icev)
```

## Find not unique value
```{r}
unique(bev_data$patent_number[duplicated(bev_data$patent_number)])
unique(fcev_data$patent_number[duplicated(fcev_data$patent_number)])
unique(icev_data$patent_number[duplicated(icev_data$patent_number)])
unique(hev_data$patent_number[duplicated(hev_data$patent_number)])

#all same

```






### Data descriptive analysis
```{R}
fcev_yearly_count <- fcev_data %>%
  mutate(year =substr(fcev_data$patent_date,1,4)) %>%
  arrange(year) %>%
  group_by(year) %>%
  count(year) 

bev_yearly_count <- bev_data %>%
  mutate(year =substr(bev_data$patent_date,1,4)) %>%
  arrange(year) %>%
  group_by(year) %>%
  count(year) 

icev_yearly_count <- icev_data %>%
  mutate(year =substr(icev_data$patent_date,1,4)) %>%
  arrange(year) %>%
  group_by(year) %>%
  count(year) 

hev_yearly_count <- hev_data %>%
  mutate(year =substr(hev_data$patent_date,1,4)) %>%
  arrange(year) %>%
  group_by(year) %>%
  count(year) 



```

```{R}






#normalize <- function(x) {
  #return((x-min(x))/(max(x)-min(x)))
#}

#fcev_yearly_count$n <- normalize(fcev_yearly_count$n)
#bev_yearly_count$n <- normalize(bev_yearly_count$n)
#icev_yearly_count$n <- normalize(icev_yearly_count$n)  
#hev_yearly_count$n <- normalize(hev_yearly_count$n)








par(mfrow=c(2,2)) 
plot(fcev_yearly_count$year , fcev_yearly_count$n)

plot(icev_yearly_count$year , icev_yearly_count$n)

plot(hev_yearly_count$year , hev_yearly_count$n)
plot(bev_yearly_count$year , bev_yearly_count$n)




```





## patent citaiton network dataset preprocessing
```{R}
library(tidyr)
library(igraph)

fcev_citedby_data <- fcev_data[,c("patent_number","citedby_patents")]
#mini_fcev_citedby_data <- fcev_citedby_data[1:100,]
# 지금 이 특허 데이터 구조 -> list 안에 list -> unlist 한번 하고 나서 또 unlist 하면 element 사라져버림


#for loop 돌릴 때, 1:20 으로 지정해줘야지..

#How to unlist 

for (i in 1:nrow(fcev_citedby_data) ){
  fcev_citedby_data[[2]][[i]] <-  unlist(fcev_citedby_data[[2]][[i]][[1]])
}



x1 <- c()
y1 <- c()

#version 1 -> no NA value
for (i in 1: nrow(fcev_citedby_data)) {
   if (!is.na(fcev_citedby_data[[2]][[i]])) {
     for (j in 1: length(fcev_citedby_data[[2]][[i]])) {
       x1 <- append(x1, fcev_citedby_data[[1]][[i]])
       y1 <- append(y1, fcev_citedby_data[[2]][[i]][[j]])
     }
   }
}


df_wo_na <- data.frame(x1, y1)
colnames(df_wo_na) <- c('patent_name_wo_na', 'forward_citation_wo_na')



#version2 -> include NA value

x <- c()
y <- c()
for (i in 1: nrow(fcev_citedby_data)) {
  if (length(fcev_citedby_data[[2]][[i]] >1 )) {
    for (j in 1: length(fcev_citedby_data[[2]][[i]])) {
      x <- append(x, fcev_citedby_data[[1]][[i]])
      y <- append(y, fcev_citedby_data[[2]][[i]][[j]])
    }
  } else {
    print(i)
      if (is.na(fcev_citedby_data[[2]][[i]])==TRUE) {
        next
      }else {
        x <- append(x, fcev_citedby_data[[1]][[i]])
        y <- append(y, fcev_citedby_data[[2]][[i]])
      }
    }
  }

df <- data.frame(x, y)
colnames(df) <- c('patent_name', 'forward_citation')



write.csv(df,file='./fcev_citedby_data.csv', row.names=FALSE)
write.csv(df_wo_na, file='./fcev_citedby_wo_na_data.csv', row.names=FALSE)

```


##Network Analysis
```{R}
library(igraph)

net <- graph_from_data_frame(df_wo_na, directed=T) 
plot(net, edge.arrow.size=.4,vertex.label=NA)

```




### Topic Modeling
## HEV Abstract Preprocessing
```{R}
#abstract<- read.csv("C:/Users/Don Hong/Desktop/yongsuk/patent/fcev_abstract.csv")
#abstract<- read.csv("C:/Users/Don Hong/Desktop/yongsuk/patent/bev_abstract.csv")
abstract<- read.csv("C:/Users/Don Hong/Desktop/yongsuk/patent/hev_abstract.csv")

docs <- VCorpus(VectorSource(abstract))
docs

to_space <- content_transformer(function(x, pattern)
  { 
    return (gsub(pattern, " ", x))
  }
)
# removing unwanted symbols
docs <- tm_map(docs, to_space, ":")
docs <- tm_map(docs, to_space, "-")
docs <- tm_map(docs, to_space, "'")
docs <- tm_map(docs, to_space, "’")
docs <- tm_map(docs, to_space, '"')
docs <- tm_map(docs, to_space, ";")
# removing punctuation
docs <- tm_map(docs, removePunctuation)
# transforming to lower case
docs <- tm_map(docs, content_transformer(tolower))
# removing numbers
docs <- tm_map(docs, removeNumbers)
# removing stop words
docs <- tm_map(docs, removeWords, stopwords())
# removing white spaces
docs <- tm_map(docs, stripWhitespace)

text_corpus_clean <- docs


#워드 클라우드 그려보기
library(wordcloud)
wordcloud(text_corpus_clean, max.words = 100, random.order = FALSE,
          
          
          colors=brewer.pal(8, "Dark2"))

#특정 단어 추가 제거
text_corpus_clean <- tm_map(text_corpus_clean,
                            removeWords, c('model'))

wordcloud(text_corpus_clean, min.freq = 200, random.order = FALSE,
          colors=brewer.pal(8, "Dark2"))




#DTM 구성
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm

#등장률 1% 미만 단어 제거 
text_dtm_concise = removeSparseTerms(text_dtm, 0.99)
text_dtm_concise




library(topicmodels)

#LDA 실행
text_lda <- LDA(text_dtm_concise, k = 4, method = "VEM", control = NULL)
text_lda

library(tidytext)

text_topics <- tidy(text_lda, matrix = "beta")
text_topics



library(ggplot2)
library(dplyr)

text_top_terms <- text_topics %>%
  group_by(topic) %>%
  # - 먼저 토픽을 기준으로 그룹화
  # - topic 1끼리 모으고, 2끼리 모으고..
  
  top_n(10, beta) %>%
  # # - top(갯수, 기준 column)
  # # - beta기준으로 상위 10개 데이터를 뽑음
  # 
  ungroup()%>%
  # # - 그룹화된걸 해제
  # # - 의미 없음
  # 
  arrange(topic, -beta)
  # # - topic끼리 모으고 
  # # - 베타를 기준으로 내림차순 정렬

text_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  # - term이라는 column추가
  # - 그래프 형식에서는 reorder를 써서 내림차순을 설정함

  ggplot(aes(term, beta, fill = factor(topic))) +
  # - aes는 그래프의 x, y축을 설정한다.
  # - term이 x축 변수명
  # - beta가 y축 변수명
  # - topic 별로 구분해줌 -> 색깔 바꺼줌

  geom_col(show.legend = FALSE) +
  # - 그룹정보(범례)는 쓰지 않음

  facet_wrap(~ topic, scales = "free") +
  # - topic 별로 묶음 , topic 1에서의 단어 모음, 2에서의 단어 모음..

  # - scales="free" 안하면
  # - topic은 나누지만 x축은 공유함

  coord_flip()
  # - 그래프 축을 전환

###LDAVIS###

library(LDAvis)
library(stringi)
library(servr)


phi <- posterior(text_lda)$terms %>% as.matrix
# posterior를 이용해 terms를 뽑아냄
# as.matrix를 통해 행렬로 만든다.

theta <- posterior(text_lda)$topics %>% as.matrix
# posterior를 이용해 topics를 뽑아냄
# as.matrix를 통해 행렬로 만든다.

vocab <- colnames(phi)
# phi가 terms집합인데 용어 각각을 열이름으로 설정함

doc_length <- c()
# c()을 통해 임의 데이터를 생성 가능
# [] 배열 느낌

for(i in 1:length(text_corpus_clean)) {
  # 우리가 코르푸스로 만든 csv의 길이만큼 for문을 돌림
  temp <- paste(text_corpus_clean[[i]]$content, collapse=" ")
  # 
  doc_length <- c(doc_length, stri_count(temp, regex='\\S+'))
  #
}
temp_frequency <- as.matrix(text_dtm_concise)
# dtm으로 정리한걸 행렬로 정리
freq_matrix <- data.frame(ST=colnames(temp_frequency),
                          Freq=colSums(temp_frequency))
# ST, Freq를 열로 만드는데 
# ST는 temp_frequency 파일의 열 이름으로
# Freq는 그 각 열 이름이 전체에서 몇번이나 나오는지 

rm(temp_frequency)


json_lda <- createJSON(phi=phi,
                       theta=theta,
                       vocab=vocab,
                       doc.length=doc_length,
                       term.frequency=freq_matrix$Freq)

serVis(json_lda, out.dir='example3', open.browser=TRUE)





#inspect(docs[[1]])

#text_corpus_clean <- docs
#findFreqTerms(text_dtm,)
#freqr <- colSums(as.matrix(text_dtm))
#length(freqr)
```

## BEV Abstract Preprocessing
```{R}

abstract<- read.csv("C:/Users/Don Hong/Desktop/yongsuk/patent/bev_abstract.csv")


docs <- VCorpus(VectorSource(abstract))
docs

to_space <- content_transformer(function(x, pattern)
  { 
    return (gsub(pattern, " ", x))
  }
)
# removing unwanted symbols
docs <- tm_map(docs, to_space, ":")
docs <- tm_map(docs, to_space, "-")
docs <- tm_map(docs, to_space, "'")
docs <- tm_map(docs, to_space, "’")
docs <- tm_map(docs, to_space, '"')
docs <- tm_map(docs, to_space, ";")
# removing punctuation
docs <- tm_map(docs, removePunctuation)
# transforming to lower case
docs <- tm_map(docs, content_transformer(tolower))
# removing numbers
docs <- tm_map(docs, removeNumbers)
# removing stop words
docs <- tm_map(docs, removeWords, stopwords())
# removing white spaces
docs <- tm_map(docs, stripWhitespace)

text_corpus_clean <- docs


#워드 클라우드 그려보기
#library(wordcloud)
#wordcloud(text_corpus_clean, max.words = 100, random.order = FALSE,
          
          
          #colors=brewer.pal(8, "Dark2"))

#특정 단어 추가 제거
text_corpus_clean <- tm_map(text_corpus_clean,
                            removeWords, c('model'))

wordcloud(text_corpus_clean, min.freq = 200, random.order = FALSE,
          colors=brewer.pal(8, "Dark2"))




#DTM 구성
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm

#등장률 1% 미만 단어 제거 
text_dtm_concise = removeSparseTerms(text_dtm, 0.99)
text_dtm_concise




library(topicmodels)

#LDA 실행
text_lda <- LDA(text_dtm_concise, k = 4, method = "VEM", control = NULL)
text_lda

library(tidytext)

text_topics <- tidy(text_lda, matrix = "beta")
text_topics



library(ggplot2)
library(dplyr)

text_top_terms <- text_topics %>%
  group_by(topic) %>%
  # - 먼저 토픽을 기준으로 그룹화
  # - topic 1끼리 모으고, 2끼리 모으고..
  
  top_n(10, beta) %>%
  # # - top(갯수, 기준 column)
  # # - beta기준으로 상위 10개 데이터를 뽑음
  # 
  ungroup()%>%
  # # - 그룹화된걸 해제
  # # - 의미 없음
  # 
  arrange(topic, -beta)
  # # - topic끼리 모으고 
  # # - 베타를 기준으로 내림차순 정렬

text_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  # - term이라는 column추가
  # - 그래프 형식에서는 reorder를 써서 내림차순을 설정함

  ggplot(aes(term, beta, fill = factor(topic))) +
  # - aes는 그래프의 x, y축을 설정한다.
  # - term이 x축 변수명
  # - beta가 y축 변수명
  # - topic 별로 구분해줌 -> 색깔 바꺼줌

  geom_col(show.legend = FALSE) +
  # - 그룹정보(범례)는 쓰지 않음

  facet_wrap(~ topic, scales = "free") +
  # - topic 별로 묶음 , topic 1에서의 단어 모음, 2에서의 단어 모음..

  # - scales="free" 안하면
  # - topic은 나누지만 x축은 공유함

  coord_flip()
  # - 그래프 축을 전환

###LDAVIS###

library(LDAvis)
library(stringi)
library(servr)


phi <- posterior(text_lda)$terms %>% as.matrix
# posterior를 이용해 terms를 뽑아냄
# as.matrix를 통해 행렬로 만든다.

theta <- posterior(text_lda)$topics %>% as.matrix
# posterior를 이용해 topics를 뽑아냄
# as.matrix를 통해 행렬로 만든다.

vocab <- colnames(phi)
# phi가 terms집합인데 용어 각각을 열이름으로 설정함

doc_length <- c()
# c()을 통해 임의 데이터를 생성 가능
# [] 배열 느낌

for(i in 1:length(text_corpus_clean)) {
  # 우리가 코르푸스로 만든 csv의 길이만큼 for문을 돌림
  temp <- paste(text_corpus_clean[[i]]$content, collapse=" ")
  # 
  doc_length <- c(doc_length, stri_count(temp, regex='\\S+'))
  #
}
temp_frequency <- as.matrix(text_dtm_concise)
# dtm으로 정리한걸 행렬로 정리
freq_matrix <- data.frame(ST=colnames(temp_frequency),
                          Freq=colSums(temp_frequency))
# ST, Freq를 열로 만드는데 
# ST는 temp_frequency 파일의 열 이름으로
# Freq는 그 각 열 이름이 전체에서 몇번이나 나오는지 

rm(temp_frequency)


json_lda <- createJSON(phi=phi,
                       theta=theta,
                       vocab=vocab,
                       doc.length=doc_length,
                       term.frequency=freq_matrix$Freq)

serVis(json_lda, out.dir='example3', open.browser=TRUE)
```

## FCEV Abstract Preprocessing
```{R}

abstract<- read.csv("C:/Users/Don Hong/Desktop/yongsuk/patent/fcev_abstract.csv")


docs <- VCorpus(VectorSource(abstract))
docs

to_space <- content_transformer(function(x, pattern)
  { 
    return (gsub(pattern, " ", x))
  }
)
# removing unwanted symbols
docs <- tm_map(docs, to_space, ":")
docs <- tm_map(docs, to_space, "-")
docs <- tm_map(docs, to_space, "'")
docs <- tm_map(docs, to_space, "’")
docs <- tm_map(docs, to_space, '"')
docs <- tm_map(docs, to_space, ";")
# removing punctuation
docs <- tm_map(docs, removePunctuation)
# transforming to lower case
docs <- tm_map(docs, content_transformer(tolower))
# removing numbers
docs <- tm_map(docs, removeNumbers)
# removing stop words
docs <- tm_map(docs, removeWords, stopwords())
# removing white spaces
docs <- tm_map(docs, stripWhitespace)

text_corpus_clean <- docs


#워드 클라우드 그려보기
#library(wordcloud)
#wordcloud(text_corpus_clean, max.words = 100, random.order = FALSE,
          
          
          #colors=brewer.pal(8, "Dark2"))

#특정 단어 추가 제거
text_corpus_clean <- tm_map(text_corpus_clean,
                            removeWords, c('model'))

wordcloud(text_corpus_clean, min.freq = 200, random.order = FALSE,
          colors=brewer.pal(8, "Dark2"))




#DTM 구성
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm

#등장률 1% 미만 단어 제거 
text_dtm_concise = removeSparseTerms(text_dtm, 0.99)
text_dtm_concise




library(topicmodels)

#LDA 실행
text_lda <- LDA(text_dtm_concise, k = 4, method = "VEM", control = NULL)
text_lda

library(tidytext)

text_topics <- tidy(text_lda, matrix = "beta")
text_topics



library(ggplot2)
library(dplyr)

text_top_terms <- text_topics %>%
  group_by(topic) %>%
  # - 먼저 토픽을 기준으로 그룹화
  # - topic 1끼리 모으고, 2끼리 모으고..
  
  top_n(10, beta) %>%
  # # - top(갯수, 기준 column)
  # # - beta기준으로 상위 10개 데이터를 뽑음
  # 
  ungroup()%>%
  # # - 그룹화된걸 해제
  # # - 의미 없음
  # 
  arrange(topic, -beta)
  # # - topic끼리 모으고 
  # # - 베타를 기준으로 내림차순 정렬

text_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  # - term이라는 column추가
  # - 그래프 형식에서는 reorder를 써서 내림차순을 설정함

  ggplot(aes(term, beta, fill = factor(topic))) +
  # - aes는 그래프의 x, y축을 설정한다.
  # - term이 x축 변수명
  # - beta가 y축 변수명
  # - topic 별로 구분해줌 -> 색깔 바꺼줌

  geom_col(show.legend = FALSE) +
  # - 그룹정보(범례)는 쓰지 않음

  facet_wrap(~ topic, scales = "free") +
  # - topic 별로 묶음 , topic 1에서의 단어 모음, 2에서의 단어 모음..

  # - scales="free" 안하면
  # - topic은 나누지만 x축은 공유함

  coord_flip()
  # - 그래프 축을 전환

###LDAVIS###

library(LDAvis)
library(stringi)
library(servr)


phi <- posterior(text_lda)$terms %>% as.matrix
# posterior를 이용해 terms를 뽑아냄
# as.matrix를 통해 행렬로 만든다.

theta <- posterior(text_lda)$topics %>% as.matrix
# posterior를 이용해 topics를 뽑아냄
# as.matrix를 통해 행렬로 만든다.

vocab <- colnames(phi)
# phi가 terms집합인데 용어 각각을 열이름으로 설정함

doc_length <- c()
# c()을 통해 임의 데이터를 생성 가능
# [] 배열 느낌

for(i in 1:length(text_corpus_clean)) {
  # 우리가 코르푸스로 만든 csv의 길이만큼 for문을 돌림
  temp <- paste(text_corpus_clean[[i]]$content, collapse=" ")
  # 
  doc_length <- c(doc_length, stri_count(temp, regex='\\S+'))
  #
}
temp_frequency <- as.matrix(text_dtm_concise)
# dtm으로 정리한걸 행렬로 정리
freq_matrix <- data.frame(ST=colnames(temp_frequency),
                          Freq=colSums(temp_frequency))
# ST, Freq를 열로 만드는데 
# ST는 temp_frequency 파일의 열 이름으로
# Freq는 그 각 열 이름이 전체에서 몇번이나 나오는지 

rm(temp_frequency)


json_lda <- createJSON(phi=phi,
                       theta=theta,
                       vocab=vocab,
                       doc.length=doc_length,
                       term.frequency=freq_matrix$Freq)

serVis(json_lda, out.dir='example3', open.browser=TRUE)
```

## ICEV Abstract Preprocessing
```{R}
abstract<- icev_data$patent_abstract

docs <- VCorpus(VectorSource(abstract))
docs

to_space <- content_transformer(function(x, pattern)
  { 
    return (gsub(pattern, " ", x))
  }
)
# removing unwanted symbols
docs <- tm_map(docs, to_space, ":")
docs <- tm_map(docs, to_space, "-")
docs <- tm_map(docs, to_space, "'")
docs <- tm_map(docs, to_space, "’")
docs <- tm_map(docs, to_space, '"')
docs <- tm_map(docs, to_space, ";")
# removing punctuation
docs <- tm_map(docs, removePunctuation)
# transforming to lower case
docs <- tm_map(docs, content_transformer(tolower))
# removing numbers
docs <- tm_map(docs, removeNumbers)
# removing stop words
docs <- tm_map(docs, removeWords, stopwords())
# removing white spaces
docs <- tm_map(docs, stripWhitespace)

text_corpus_clean <- docs


#워드 클라우드 그려보기
library(wordcloud)
wordcloud(text_corpus_clean, max.words = 100, random.order = FALSE,
          
          
          colors=brewer.pal(8, "Dark2"))

#특정 단어 추가 제거
text_corpus_clean <- tm_map(text_corpus_clean,
                            removeWords, c('model'))

wordcloud(text_corpus_clean, min.freq = 200, random.order = FALSE,
          colors=brewer.pal(8, "Dark2"))




#DTM 구성
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm

#등장률 1% 미만 단어 제거 
text_dtm_concise = removeSparseTerms(text_dtm, 0.99)
text_dtm_concise




library(topicmodels)

#LDA 실행
text_lda <- LDA(text_dtm_concise, k = 4, method = "VEM", control = NULL)
text_lda

library(tidytext)

text_topics <- tidy(text_lda, matrix = "beta")
text_topics



library(ggplot2)
library(dplyr)

text_top_terms <- text_topics %>%
  group_by(topic) %>%
  # - 먼저 토픽을 기준으로 그룹화
  # - topic 1끼리 모으고, 2끼리 모으고..
  
  top_n(10, beta) %>%
  # # - top(갯수, 기준 column)
  # # - beta기준으로 상위 10개 데이터를 뽑음
  # 
  ungroup()%>%
  # # - 그룹화된걸 해제
  # # - 의미 없음
  # 
  arrange(topic, -beta)
  # # - topic끼리 모으고 
  # # - 베타를 기준으로 내림차순 정렬

text_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  # - term이라는 column추가
  # - 그래프 형식에서는 reorder를 써서 내림차순을 설정함

  ggplot(aes(term, beta, fill = factor(topic))) +
  # - aes는 그래프의 x, y축을 설정한다.
  # - term이 x축 변수명
  # - beta가 y축 변수명
  # - topic 별로 구분해줌 -> 색깔 바꺼줌

  geom_col(show.legend = FALSE) +
  # - 그룹정보(범례)는 쓰지 않음

  facet_wrap(~ topic, scales = "free") +
  # - topic 별로 묶음 , topic 1에서의 단어 모음, 2에서의 단어 모음..

  # - scales="free" 안하면
  # - topic은 나누지만 x축은 공유함

  coord_flip()
  # - 그래프 축을 전환

###LDAVIS###

library(LDAvis)
library(stringi)
library(servr)


phi <- posterior(text_lda)$terms %>% as.matrix
# posterior를 이용해 terms를 뽑아냄
# as.matrix를 통해 행렬로 만든다.

theta <- posterior(text_lda)$topics %>% as.matrix
# posterior를 이용해 topics를 뽑아냄
# as.matrix를 통해 행렬로 만든다.

vocab <- colnames(phi)
# phi가 terms집합인데 용어 각각을 열이름으로 설정함

doc_length <- c()
# c()을 통해 임의 데이터를 생성 가능
# [] 배열 느낌

for(i in 1:length(text_corpus_clean)) {
  # 우리가 코르푸스로 만든 csv의 길이만큼 for문을 돌림
  temp <- paste(text_corpus_clean[[i]]$content, collapse=" ")
  # 
  doc_length <- c(doc_length, stri_count(temp, regex='\\S+'))
  #
}
temp_frequency <- as.matrix(text_dtm_concise)
# dtm으로 정리한걸 행렬로 정리
freq_matrix <- data.frame(ST=colnames(temp_frequency),
                          Freq=colSums(temp_frequency))
# ST, Freq를 열로 만드는데 
# ST는 temp_frequency 파일의 열 이름으로
# Freq는 그 각 열 이름이 전체에서 몇번이나 나오는지 

rm(temp_frequency)


json_lda <- createJSON(phi=phi,
                       theta=theta,
                       vocab=vocab,
                       doc.length=doc_length,
                       term.frequency=freq_matrix$Freq)

serVis(json_lda, out.dir='example3', open.browser=TRUE)





#inspect(docs[[1]])

#text_corpus_clean <- docs
#findFreqTerms(text_dtm,)
#freqr <- colSums(as.matrix(text_dtm))
#length(freqr)
```




## STM modeling for HEV , BEV , FCEV, ICEV
```{r}
#1. FCEV
abstract<- read.csv("C:/Users/Don Hong/Desktop/yongsuk/patent/fcev_abstract.csv")




#Corpus 구성
text_corpus <- VCorpus(VectorSource(abstract))
text_corpus

#####전처리 시작 (tm)
#대문자 -> 소문자
text_corpus_clean <- tm_map(text_corpus,
                            content_transformer(tolower))

#stemming
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)

#숫자 제거
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)

#범용어 제거
text_corpus_clean <- tm_map(text_corpus_clean,
                            removeWords, stopwords())

#기호 제거
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)

#공백 제거
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)


#워드 클라우드 그려보기
library(wordcloud)
wordcloud(text_corpus_clean, max.words = 100, random.order = FALSE,
          colors=brewer.pal(8, "Dark2"))

#특정 단어 추가 제거
text_corpus_clean <- tm_map(text_corpus_clean,
                            removeWords, c('model'))

wordcloud(text_corpus_clean, min.freq = 200, random.order = FALSE,
          colors=brewer.pal(8, "Dark2"))




#DTM 구성
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm

#등장률 1% 미만 단어 제거 
text_dtm_concise = removeSparseTerms(text_dtm, 0.99)
text_dtm_concise




library(topicmodels)

################################################
################################################
################################################
#2. STM ########################################

#install.packages("stm")
#install.packages("stopwords")
library(stm)
library(stopwords)

mydata <- abstract

#STM: 전처리가 쉬움 ! 

#범용어 제거
stwds <- stopwords(language = "en", source = "smart")
custom = c('model')
csw <- c(stwds, custom)

#전처리 1차
mypreprocess <- textProcessor(mydata, metadata = mydata, customstopwords = csw,
                                 stem = TRUE, lowercase = TRUE, removestopwords = TRUE, removenumbers = TRUE, removepunctuation = TRUE
                                 , wordLengths = c(3,Inf))

#전처리 2차 (단어 기준 컷)
myout <-prepDocuments(mypreprocess$documents,
                         mypreprocess$vocab, mypreprocess$meta,
                         lower.thresh = 50)

#단어 확인
myout$vocab





#STM 토픽 수 설정
kno = 4
#STM 분석
mystm <- stm(myout$documents, myout$vocab, K=kno,
              prevalence = ~ year ,
              data=myout$meta,
              seed = 16, init.type = "Spectral")


#토픽 결과 관찰
labelTopics(mystm, topics=1:kno, n=10)






#회귀분석 수행
myresult <- estimateEffect(c(1:kno) ~ year, mystm, myout$meta)
summary(myresult)


####LDAvis에 적용

toLDAvis(mystm, myout$documents, R=50, plot.opts = list(xlab = "PC-1", ylab = "PC-2"), 
         lambda.step = 0.05, 
         reorder.topics = FALSE)

toLDAvis(mystm, myout$documents)

#2x2 
par(mfrow=c(2,2))
mystm.labels = labelTopics(mystm, topics=1:kno)

#시각화 
for (i in seq_along(sample(1:kno, size = kno))){
  plot(myresult, "year", method="continuous",topics = i, main = paste0(mystm.labels$prob[i,1:3], collapse = ", "),
       printlegend = F)
}





```

## Network analysis
```{r}
edges <-
  fcev_data$cited_patents %>%
    semi_join(x = ., y = ., by = c("cited_patent_number" = "patent_number")) %>%
    set_colnames(c("from", "to"))

nodes <-
  fcev_data$patents %>%
    mutate(
      id = patent_number,
      label = patent_number,
      title = patent_title
    )


visNetwork(
  nodes = nodes, edges = edges, height = "400px", width = "100%",
  main = "Citations among FCEV patents"
) %>%
  visEdges(arrows = list(to = list(enabled = TRUE))) %>%
  visIgraphLayout()

```
